{"cells":[{"cell_type":"markdown","source":["# Problem Statement"],"metadata":{"id":"t6d77yoag0Ph"}},{"cell_type":"markdown","source":["1. üõ†Ô∏è The overall problem is to develop a text classification system using a pre-trained DistilBert model that can accurately predict categories based on textual input.\n","\n","2. üìù The input feature, denoted as 'X', consists of raw text strings sourced from a dataset, which the model processes to predict categorical labels.\n","\n","3. üéØ The target variable, referred to as 'label', represents the actual categories of the text, which are used to train the model and evaluate its accuracy.\n","\n","4. üìä The model's performance is assessed through metrics such as the confusion matrix and accuracy, comparing the predicted labels against the actual labels.\n","\n","5. üîß The project involves not only fine-tuning a pre-trained language model on a specific dataset but also validating its effectiveness on both a smaller sample and a larger subset to ensure robustness and scalability of the predictions."],"metadata":{"id":"IDc_Falyg2N2"}},{"cell_type":"markdown","source":["#Install the necessary libraries"],"metadata":{"id":"PhSY4bpFhDwB"}},{"cell_type":"markdown","source":["\n","\n","1. **Install Accelerate**:\n","2. **Install Transformers and Datasets**:\n","3. **Upgrade PyArrow**:\n","\n","Import Libraries\n","\n","1. **Import Standard Data Science Libraries**:\n","3. **Import PyTorch and Transformers for NLP**:\n","\n"],"metadata":{"id":"Sp057gy7Ymd-"}},{"cell_type":"code","source":["## Write your code here"],"metadata":{"id":"SpqZW_YdceRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OAYvArjfceXD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYFE5O8RB4MD"},"source":["# Import Emotions Data. Target variable is 'Label'"]},{"cell_type":"markdown","source":["Import the data from https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Final_Emotion_Data/five_emotions_data.csv\n","\n","Display the first few rows of the dataset to understand its structure"],"metadata":{"id":"0epYOisOaa9L"}},{"cell_type":"code","source":["## Write your code here"],"metadata":{"id":"IMYlACBncdUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SvRS2Enzcdgq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqmjOcoOB-ec"},"source":["# Use distilbert model without finetunung"]},{"cell_type":"markdown","source":["Here are concise instructions to write and execute the Python code you provided, which utilizes a pre-trained DistilBERT model from the `transformers` library for text classification without fine-tuning:\n","\n","Step 1: Install and Import Libraries\n","1. **Install the necessary Python packages** if they're not already installed:\n","\n","2. **Import the libraries** in your Python script:\n","\n","\n"," Step 2: Set Up the DistilBERT Model\n","1. **Initialize the DistilBERT pipeline** for text classification. Ensure you have GPU support (CUDA) if available:\n","\n","\n"," Step 3: Prepare and Predict with Your Dataset\n","1. **Sample and preprocess the dataset**:\n","   - Assuming `emotions_data` is already loaded:\n","\n","2. **Classify text** using the model and extract predictions:\n","\n","\n","Step 4: Evaluate Model Performance\n","1. **Calculate and print the confusion matrix** and accuracy:\n","\n","\n"],"metadata":{"id":"ipqmpa1GcgI0"}},{"cell_type":"code","source":["## Write your code here"],"metadata":{"id":"-5wt0awRcfki"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BJkxMer_wjr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WbHzWMRDiRW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8IoArWg3ksvC"},"source":["# Finetuning the model with our data\n"]},{"cell_type":"markdown","source":["1. Begin by importing necessary libraries including `DistilBertTokenizer` and `DistilBertForSequenceClassification` from `transformers`, `load_dataset`, `DatasetDict`, `ClassLabel`, and `Dataset` from `datasets`, `pandas`, `train_test_split` from `sklearn.model_selection`, and `torch`.\n","\n","2. Load your sample data into a pandas DataFrame, `sample_data`.\n","\n","3. Convert the DataFrame into a `Dataset` object using `Dataset.from_pandas()` method.\n","\n","4. Split the data into training and testing sets using the `train_test_split` method from the `datasets` library, setting aside 20% of the data for testing.\n","\n","5. Create a `DatasetDict` to organize the datasets into 'train' and 'test' subsets.\n","\n","6. Load the `DistilBertTokenizer` from the `transformers` library using the pretrained 'distilbert-base-uncased' model.\n","\n","7. Set the tokenizer's padding token to the end-of-sequence token and update the padding token ID accordingly. Add a special padding token '[PAD]' to the tokenizer.\n","\n","8. Define a function, `tokenize_function`, that tokenizes the texts in the dataset. Ensure the texts are padded to a maximum length of 100 characters and truncated if necessary.\n","\n","9. Apply the `tokenize_function` to the datasets using the `.map()` method with `batched=True` to process batches of texts at once."],"metadata":{"id":"mLsQ4E6Pdx0K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZT4RxigoO3a"},"outputs":[],"source":["#Write your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cXivZhOxjQtA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khB_bZv0lcXL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"l_Q_vJOISAtX"},"source":["## Load and Train the model"]},{"cell_type":"markdown","source":["1. Load the `DistilBertForSequenceClassification` model from the `transformers` library, using the pretrained 'distilbert-base-uncased' model. Specify `num_labels` based on your classification needs and set `pad_token_id` to the tokenizer's end-of-sequence token ID.\n","\n","2. Set up the training configuration using `TrainingArguments`. Define an `output_dir` for storing training results, set `num_train_epochs` for the number of training epochs, specify a `logging_dir` for training logs, and choose `evaluation_strategy` as 'epoch' to perform evaluations at the end of each epoch.\n","\n","3. Initialize a `Trainer` object with the model, training arguments, and datasets for training and evaluation. Specify the training dataset using `tokenized_datasets['train']` and the evaluation dataset as `tokenized_datasets['test']`.\n","\n","4. Begin the training process by invoking the `train()` method on the `Trainer`.\n","\n","5. Specify a directory where you want to save both the trained model and the tokenizer, e.g., \"./distilbert_finetuned\".\n","\n","6. Save the trained model and tokenizer to the specified directory using the `save_pretrained()` method for both the model and tokenizer.\n","\n","7. Additionally, save the model to another directory or under another name using the `save_model()` method of the `Trainer`.\n","\n","8. Compress the model directory into a ZIP file using a zip utility, specifying the target directory and the name for the ZIP file, e.g., \"distilbert_finetuned_10k.zip\"."],"metadata":{"id":"u2-GPqu0eKeZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbMzSADKvCtB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIVONC0H7YZc"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"SpEpK61vfwE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Make Predictions"],"metadata":{"id":"cEoY5fV6eh7x"}},{"cell_type":"markdown","source":["1. Define a function, `make_prediction`, which takes a single text string as input. Inside this function:\n","   - Tokenize the text using your pretrained tokenizer. Specify that tensors should be returned as PyTorch tensors (`return_tensors=\"pt\"`).\n","   - Send the tokenized inputs to a CUDA device for GPU acceleration.\n","   - Pass the inputs to your pretrained model and obtain the outputs.\n","   - Extract the logits from the outputs and determine the predicted class by finding the index of the maximum logit value.\n","   - Convert the prediction tensor from GPU to CPU and transform it into a numpy array.\n","   - Return the prediction.\n","\n","2. Apply the `make_prediction` function to the \"Text\" column of your `sample_data` DataFrame, extracting the first element of the returned array to a new column named `finetuned_predicted`.\n","\n","3. Calculate the confusion matrix using `confusion_matrix` from `sklearn.metrics`, comparing the actual labels and the predicted labels from your DataFrame. Print the resulting confusion matrix.\n","\n","4. Compute and print the accuracy of your predictions by dividing the sum of the diagonal elements of the confusion matrix by the total number of predictions.\n","\n","5. Select a large subset (e.g., 40,000 samples) from another dataset (assumed to be `emotions_data` here), using a fixed random state for reproducibility.\n","\n","6. Apply the `make_prediction` function to the \"Text\" column of this large dataset, storing the results in a new column named `finetuned_predicted`.\n","\n","7. Calculate and print the confusion matrix for this larger dataset to compare the predicted labels against the actual labels.\n","\n","8. Compute and print the accuracy for this dataset in the same manner as described above."],"metadata":{"id":"52_PJUyufscc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8BsIrTyWc72"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99Z7s9P-C-hg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9Iq2KJD-AJ5"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"toc_visible":true,"mount_file_id":"11iz8gq4OmG_O_UKvtbXV3zsCKFVrPCP9","authorship_tag":"ABX9TyMbuPYCFyupNORbgBF5Do12"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}